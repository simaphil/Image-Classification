{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbV1AJxsj5OR6tQxBOMdzt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simaphil/Image-Classification/blob/master/image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GieltfW7giOW",
        "outputId": "57a8d43b-5e87-4911-dd03-c27a6b08d350"
      },
      "source": [
        "#Import the dataset to a new file in tmp called weather\n",
        "\n",
        "!mkdir '/tmp/weather'\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "  https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/4drtyfjtfy-1.zip -O \\\n",
        "  /tmp/weather/weather.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-10 06:35:03--  https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/4drtyfjtfy-1.zip\n",
            "Resolving md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)... 52.218.106.144\n",
            "Connecting to md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)|52.218.106.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95592747 (91M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/weather/weather.zip’\n",
            "\n",
            "/tmp/weather/weathe 100%[===================>]  91.16M  11.0MB/s    in 10s     \n",
            "\n",
            "2021-06-10 06:35:14 (8.78 MB/s) - ‘/tmp/weather/weather.zip’ saved [95592747/95592747]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFDawY8AoOYo"
      },
      "source": [
        "# Part 1: Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjoM6JVY7qu8"
      },
      "source": [
        "#Extract the data from the zip file\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/weather/weather.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/weather')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = '/tmp/weather/dataset2.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/weather')\n",
        "zip_ref.close()\n",
        "\n",
        "#Get list of all files in directory\n",
        "files=os.listdir('/tmp/weather/dataset2')\n",
        "\n",
        "# The data has 1125 images:\n",
        "#   300 cloudy,\n",
        "#   215 rain,\n",
        "#   253 shine,\n",
        "#   357 sunrise\n",
        "\n",
        "#Create folders for each category\n",
        "!mkdir '/tmp/weather/dataset2/cloudy'\n",
        "!mkdir '/tmp/weather/dataset2/rain'\n",
        "!mkdir '/tmp/weather/dataset2/shine'\n",
        "!mkdir '/tmp/weather/dataset2/sunrise'\n",
        "#Move each image to the correct file:\n",
        "for i in range(len(files)):\n",
        "  p='/tmp/weather/dataset2/'+files[i] #Current image's path\n",
        "  if(files[i][:4]=='rain'): #If the file name starts with rain move it to rain folder\n",
        "    !mv {p} '/tmp/weather/dataset2/rain'\n",
        "  else:\n",
        "    if(files[i][:4]=='clou'):#If file name begins with clou move to cloudy folder\n",
        "       !mv {p} '/tmp/weather/dataset2/cloudy'\n",
        "    else:\n",
        "      if(files[i][:4]=='shin'):#If file name begins with shin move to shine folder\n",
        "          !mv {p} '/tmp/weather/dataset2/shine'\n",
        "      else: #else move to sunrise folder\n",
        "        !mv {p} '/tmp/weather/dataset2/sunrise'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeNsVCxebOEQ",
        "outputId": "b294e215-9cd4-4abc-ba5f-c219a132696d"
      },
      "source": [
        "os.listdir('/tmp/weather/dataset2/cloudy')\n",
        "#Show that list of files in folder is ordered randomly\n",
        "#so that in future when I traverse it to split the data, it is done randomly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cloudy238.jpg',\n",
              " 'cloudy101.jpg',\n",
              " 'cloudy194.jpg',\n",
              " 'cloudy138.jpg',\n",
              " 'cloudy84.jpg',\n",
              " 'cloudy17.jpg',\n",
              " 'cloudy127.jpg',\n",
              " 'cloudy167.jpg',\n",
              " 'cloudy203.jpg',\n",
              " 'cloudy287.jpg',\n",
              " 'cloudy285.jpg',\n",
              " 'cloudy164.jpg',\n",
              " 'cloudy208.jpg',\n",
              " 'cloudy280.jpg',\n",
              " 'cloudy297.jpg',\n",
              " 'cloudy233.jpg',\n",
              " 'cloudy292.jpg',\n",
              " 'cloudy266.jpg',\n",
              " 'cloudy291.jpg',\n",
              " 'cloudy126.jpg',\n",
              " 'cloudy171.jpg',\n",
              " 'cloudy78.jpg',\n",
              " 'cloudy247.jpg',\n",
              " 'cloudy53.jpg',\n",
              " 'cloudy49.jpg',\n",
              " 'cloudy135.jpg',\n",
              " 'cloudy12.jpg',\n",
              " 'cloudy70.jpg',\n",
              " 'cloudy119.jpg',\n",
              " 'cloudy97.jpg',\n",
              " 'cloudy213.jpg',\n",
              " 'cloudy100.jpg',\n",
              " 'cloudy281.jpg',\n",
              " 'cloudy105.jpg',\n",
              " 'cloudy185.jpg',\n",
              " 'cloudy7.jpg',\n",
              " 'cloudy212.jpg',\n",
              " 'cloudy137.jpg',\n",
              " 'cloudy146.jpg',\n",
              " 'cloudy174.jpg',\n",
              " 'cloudy24.jpg',\n",
              " 'cloudy114.jpg',\n",
              " 'cloudy44.jpg',\n",
              " 'cloudy151.jpg',\n",
              " 'cloudy67.jpg',\n",
              " 'cloudy152.jpg',\n",
              " 'cloudy56.jpg',\n",
              " 'cloudy58.jpg',\n",
              " 'cloudy244.jpg',\n",
              " 'cloudy123.jpg',\n",
              " 'cloudy112.jpg',\n",
              " 'cloudy48.jpg',\n",
              " 'cloudy28.jpg',\n",
              " 'cloudy196.jpg',\n",
              " 'cloudy242.jpg',\n",
              " 'cloudy161.jpg',\n",
              " 'cloudy293.jpg',\n",
              " 'cloudy220.jpg',\n",
              " 'cloudy66.jpg',\n",
              " 'cloudy156.jpg',\n",
              " 'cloudy27.jpg',\n",
              " 'cloudy14.jpg',\n",
              " 'cloudy186.jpg',\n",
              " 'cloudy277.jpg',\n",
              " 'cloudy154.jpg',\n",
              " 'cloudy252.jpg',\n",
              " 'cloudy107.jpg',\n",
              " 'cloudy128.jpg',\n",
              " 'cloudy176.jpg',\n",
              " 'cloudy22.jpg',\n",
              " 'cloudy37.jpg',\n",
              " 'cloudy172.jpg',\n",
              " 'cloudy260.jpg',\n",
              " 'cloudy159.jpg',\n",
              " 'cloudy111.jpg',\n",
              " 'cloudy118.jpg',\n",
              " 'cloudy157.jpg',\n",
              " 'cloudy88.jpg',\n",
              " 'cloudy276.jpg',\n",
              " 'cloudy46.jpg',\n",
              " 'cloudy4.jpg',\n",
              " 'cloudy192.jpg',\n",
              " 'cloudy275.jpg',\n",
              " 'cloudy98.jpg',\n",
              " 'cloudy95.jpg',\n",
              " 'cloudy229.jpg',\n",
              " 'cloudy217.jpg',\n",
              " 'cloudy39.jpg',\n",
              " 'cloudy226.jpg',\n",
              " 'cloudy259.jpg',\n",
              " 'cloudy189.jpg',\n",
              " 'cloudy195.jpg',\n",
              " 'cloudy10.jpg',\n",
              " 'cloudy256.jpg',\n",
              " 'cloudy116.jpg',\n",
              " 'cloudy143.jpg',\n",
              " 'cloudy5.jpg',\n",
              " 'cloudy71.jpg',\n",
              " 'cloudy168.jpg',\n",
              " 'cloudy59.jpg',\n",
              " 'cloudy109.jpg',\n",
              " 'cloudy210.jpg',\n",
              " 'cloudy263.jpg',\n",
              " 'cloudy237.jpg',\n",
              " 'cloudy243.jpg',\n",
              " 'cloudy214.jpg',\n",
              " 'cloudy190.jpg',\n",
              " 'cloudy141.jpg',\n",
              " 'cloudy33.jpg',\n",
              " 'cloudy160.jpg',\n",
              " 'cloudy258.jpg',\n",
              " 'cloudy125.jpg',\n",
              " 'cloudy25.jpg',\n",
              " 'cloudy300.jpg',\n",
              " 'cloudy8.jpg',\n",
              " 'cloudy82.jpg',\n",
              " 'cloudy178.jpg',\n",
              " 'cloudy188.jpg',\n",
              " 'cloudy199.jpg',\n",
              " 'cloudy115.jpg',\n",
              " 'cloudy261.jpg',\n",
              " 'cloudy69.jpg',\n",
              " 'cloudy91.jpg',\n",
              " 'cloudy61.jpg',\n",
              " 'cloudy248.jpg',\n",
              " 'cloudy106.jpg',\n",
              " 'cloudy38.jpg',\n",
              " 'cloudy2.jpg',\n",
              " 'cloudy132.jpg',\n",
              " 'cloudy249.jpg',\n",
              " 'cloudy278.jpg',\n",
              " 'cloudy15.jpg',\n",
              " 'cloudy200.jpg',\n",
              " 'cloudy173.jpg',\n",
              " 'cloudy134.jpg',\n",
              " 'cloudy117.jpg',\n",
              " 'cloudy170.jpg',\n",
              " 'cloudy279.jpg',\n",
              " 'cloudy149.jpg',\n",
              " 'cloudy139.jpg',\n",
              " 'cloudy207.jpg',\n",
              " 'cloudy294.jpg',\n",
              " 'cloudy219.jpg',\n",
              " 'cloudy216.jpg',\n",
              " 'cloudy144.jpg',\n",
              " 'cloudy225.jpg',\n",
              " 'cloudy131.jpg',\n",
              " 'cloudy147.jpg',\n",
              " 'cloudy264.jpg',\n",
              " 'cloudy50.jpg',\n",
              " 'cloudy179.jpg',\n",
              " 'cloudy218.jpg',\n",
              " 'cloudy268.jpg',\n",
              " 'cloudy223.jpg',\n",
              " 'cloudy250.jpg',\n",
              " 'cloudy230.jpg',\n",
              " 'cloudy80.jpg',\n",
              " 'cloudy23.jpg',\n",
              " 'cloudy206.jpg',\n",
              " 'cloudy257.jpg',\n",
              " 'cloudy63.jpg',\n",
              " 'cloudy41.jpg',\n",
              " 'cloudy295.jpg',\n",
              " 'cloudy165.jpg',\n",
              " 'cloudy299.jpg',\n",
              " 'cloudy122.jpg',\n",
              " 'cloudy162.jpg',\n",
              " 'cloudy198.jpg',\n",
              " 'cloudy193.jpg',\n",
              " 'cloudy221.jpg',\n",
              " 'cloudy145.jpg',\n",
              " 'cloudy288.jpg',\n",
              " 'cloudy55.jpg',\n",
              " 'cloudy11.jpg',\n",
              " 'cloudy16.jpg',\n",
              " 'cloudy182.jpg',\n",
              " 'cloudy235.jpg',\n",
              " 'cloudy163.jpg',\n",
              " 'cloudy121.jpg',\n",
              " 'cloudy36.jpg',\n",
              " 'cloudy204.jpg',\n",
              " 'cloudy65.jpg',\n",
              " 'cloudy102.jpg',\n",
              " 'cloudy73.jpg',\n",
              " 'cloudy6.jpg',\n",
              " 'cloudy74.jpg',\n",
              " 'cloudy120.jpg',\n",
              " 'cloudy175.jpg',\n",
              " 'cloudy42.jpg',\n",
              " 'cloudy215.jpg',\n",
              " 'cloudy45.jpg',\n",
              " 'cloudy267.jpg',\n",
              " 'cloudy43.jpg',\n",
              " 'cloudy29.jpg',\n",
              " 'cloudy62.jpg',\n",
              " 'cloudy239.jpg',\n",
              " 'cloudy240.jpg',\n",
              " 'cloudy130.jpg',\n",
              " 'cloudy86.jpg',\n",
              " 'cloudy124.jpg',\n",
              " 'cloudy205.jpg',\n",
              " 'cloudy296.jpg',\n",
              " 'cloudy290.jpg',\n",
              " 'cloudy227.jpg',\n",
              " 'cloudy34.jpg',\n",
              " 'cloudy155.jpg',\n",
              " 'cloudy92.jpg',\n",
              " 'cloudy169.jpg',\n",
              " 'cloudy236.jpg',\n",
              " 'cloudy201.jpg',\n",
              " 'cloudy265.jpg',\n",
              " 'cloudy274.jpg',\n",
              " 'cloudy251.jpg',\n",
              " 'cloudy180.jpg',\n",
              " 'cloudy13.jpg',\n",
              " 'cloudy75.jpg',\n",
              " 'cloudy232.jpg',\n",
              " 'cloudy94.jpg',\n",
              " 'cloudy246.jpg',\n",
              " 'cloudy298.jpg',\n",
              " 'cloudy52.jpg',\n",
              " 'cloudy20.jpg',\n",
              " 'cloudy254.jpg',\n",
              " 'cloudy284.jpg',\n",
              " 'cloudy197.jpg',\n",
              " 'cloudy19.jpg',\n",
              " 'cloudy228.jpg',\n",
              " 'cloudy103.jpg',\n",
              " 'cloudy282.jpg',\n",
              " 'cloudy142.jpg',\n",
              " 'cloudy81.jpg',\n",
              " 'cloudy40.jpg',\n",
              " 'cloudy110.jpg',\n",
              " 'cloudy255.jpg',\n",
              " 'cloudy83.jpg',\n",
              " 'cloudy211.jpg',\n",
              " 'cloudy181.jpg',\n",
              " 'cloudy150.jpg',\n",
              " 'cloudy99.jpg',\n",
              " 'cloudy209.jpg',\n",
              " 'cloudy104.jpg',\n",
              " 'cloudy231.jpg',\n",
              " 'cloudy68.jpg',\n",
              " 'cloudy202.jpg',\n",
              " 'cloudy64.jpg',\n",
              " 'cloudy1.jpg',\n",
              " 'cloudy35.jpg',\n",
              " 'cloudy191.jpg',\n",
              " 'cloudy60.jpg',\n",
              " 'cloudy273.jpg',\n",
              " 'cloudy72.jpg',\n",
              " 'cloudy77.jpg',\n",
              " 'cloudy222.jpg',\n",
              " 'cloudy108.jpg',\n",
              " 'cloudy286.jpg',\n",
              " 'cloudy51.jpg',\n",
              " 'cloudy140.jpg',\n",
              " 'cloudy32.jpg',\n",
              " 'cloudy148.jpg',\n",
              " 'cloudy262.jpg',\n",
              " 'cloudy241.jpg',\n",
              " 'cloudy89.jpg',\n",
              " 'cloudy76.jpg',\n",
              " 'cloudy184.jpg',\n",
              " 'cloudy47.jpg',\n",
              " 'cloudy87.jpg',\n",
              " 'cloudy166.jpg',\n",
              " 'cloudy26.jpg',\n",
              " 'cloudy93.jpg',\n",
              " 'cloudy187.jpg',\n",
              " 'cloudy133.jpg',\n",
              " 'cloudy183.jpg',\n",
              " 'cloudy269.jpg',\n",
              " 'cloudy3.jpg',\n",
              " 'cloudy21.jpg',\n",
              " 'cloudy31.jpg',\n",
              " 'cloudy283.jpg',\n",
              " 'cloudy224.jpg',\n",
              " 'cloudy158.jpg',\n",
              " 'cloudy85.jpg',\n",
              " 'cloudy234.jpg',\n",
              " 'cloudy30.jpg',\n",
              " 'cloudy153.jpg',\n",
              " 'cloudy270.jpg',\n",
              " 'cloudy54.jpg',\n",
              " 'cloudy79.jpg',\n",
              " 'cloudy253.jpg',\n",
              " 'cloudy177.jpg',\n",
              " 'cloudy57.jpg',\n",
              " 'cloudy271.jpg',\n",
              " 'cloudy289.jpg',\n",
              " 'cloudy136.jpg',\n",
              " 'cloudy113.jpg',\n",
              " 'cloudy129.jpg',\n",
              " 'cloudy90.jpg',\n",
              " 'cloudy18.jpg',\n",
              " 'cloudy245.jpg',\n",
              " 'cloudy96.jpg',\n",
              " 'cloudy9.jpg',\n",
              " 'cloudy272.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8akL7F75exj"
      },
      "source": [
        "#Splitting the data to train and validation: I chose to do 80%/20% because it is common practice\n",
        "\n",
        "!mkdir '/tmp/weather/dataset2/train' #Create a folder for train data\n",
        "!mkdir '/tmp/weather/dataset2/validation' #Create a folder for validation data\n",
        "\n",
        "#Create folders for each category inside train folder (this makes it supervised learning)\n",
        "!mkdir '/tmp/weather/dataset2/train/cloudy'\n",
        "!mkdir '/tmp/weather/dataset2/train/rain'\n",
        "!mkdir '/tmp/weather/dataset2/train/shine'\n",
        "!mkdir '/tmp/weather/dataset2/train/sunrise'\n",
        "\n",
        "#Get the lists of files in each category\n",
        "f_cloudy=os.listdir('/tmp/weather/dataset2/cloudy')\n",
        "f_rain=os.listdir('/tmp/weather/dataset2/rain')\n",
        "f_shine=os.listdir('/tmp/weather/dataset2/shine')\n",
        "f_sunrise=os.listdir('/tmp/weather/dataset2/sunrise')\n",
        "\n",
        "#Creating the training- 80% from each category\n",
        "#(I showed above that the lists are ordered randomly)\n",
        "for i in range(round(len(f_cloudy)*0.8)): #80% of cloudy images will be moved to train\n",
        "#for i in range(round(len(f_cloudy)*0.7)): #Changing the slpit to 70%/30% to find best results\n",
        "  p='/tmp/weather/dataset2/cloudy/'+f_cloudy[i]\n",
        "  !mv {p} '/tmp/weather/dataset2/train/cloudy'\n",
        "\n",
        "for i in range(round(len(f_rain)*0.8)): #80% of rain images will be moved to train\n",
        "#for i in range(round(len(f_rain)*0.7)):  #Changing the slpit to 70%/30% to find best results\n",
        "  p='/tmp/weather/dataset2/rain/'+f_rain[i]\n",
        "  !mv {p} '/tmp/weather/dataset2/train/rain'\n",
        "\n",
        "for i in range(round(len(f_shine)*0.8)): #80% of shine images will be moved to train\n",
        "#for i in range(round(len(f_shine)*0.7)):  #Changing the slpit to 70%/30% to find best results\n",
        "  p='/tmp/weather/dataset2/shine/'+f_shine[i]\n",
        "  !mv {p} '/tmp/weather/dataset2/train/shine'\n",
        "\n",
        "for i in range(round(len(f_sunrise)*0.8)): #80% of sunrise images will be moved to train\n",
        "#for i in range(round(len(f_sunrise)*0.7)):  #Changing the slpit to 70%/30% to find best results\n",
        "  p='/tmp/weather/dataset2/sunrise/'+f_sunrise[i]\n",
        "  !mv {p} '/tmp/weather/dataset2/train/sunrise'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFC2Q0a3kBEQ"
      },
      "source": [
        "#Creating the testing- 20% from each category\n",
        "\n",
        "#Create folders for each category inside validation folder (this makes it supervised learning)\n",
        "!mkdir '/tmp/weather/dataset2/validation/cloudy'\n",
        "!mkdir '/tmp/weather/dataset2/validation/rain'\n",
        "!mkdir '/tmp/weather/dataset2/validation/shine'\n",
        "!mkdir '/tmp/weather/dataset2/validation/sunrise'\n",
        "\n",
        "#Get the lists of files in each category\n",
        "f_cloudy=os.listdir('/tmp/weather/dataset2/cloudy')\n",
        "f_rain=os.listdir('/tmp/weather/dataset2/rain')\n",
        "f_shine=os.listdir('/tmp/weather/dataset2/shine')\n",
        "f_sunrise=os.listdir('/tmp/weather/dataset2/sunrise')\n",
        "\n",
        "for i in f_cloudy: #The images left in cloudy folder (20%) will be moved to validation folder\n",
        "  p='/tmp/weather/dataset2/cloudy/'+i\n",
        "  !mv {p} '/tmp/weather/dataset2/validation/cloudy'\n",
        "\n",
        "for i in f_rain:\n",
        "  p='/tmp/weather/dataset2/rain/'+i #The images left in rain folder (20%) will be moved to validation folder\n",
        "  !mv {p} '/tmp/weather/dataset2/validation/rain'\n",
        "\n",
        "for i in f_shine:\n",
        "  p='/tmp/weather/dataset2/shine/'+i #The images left in shine folder (20%) will be moved to validation folder\n",
        "  !mv {p} '/tmp/weather/dataset2/validation/shine'\n",
        "\n",
        "for i in f_sunrise:\n",
        "  p='/tmp/weather/dataset2/sunrise/'+i #The images left in sunrise folder (20%) will be moved to validation folder\n",
        "  !mv {p} '/tmp/weather/dataset2/validation/sunrise'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVDGYkPvhVVi"
      },
      "source": [
        "train_dir ='/tmp/weather/dataset2/train'\n",
        "validation_dir = '/tmp/weather/dataset2/validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Xf92WAqCM0",
        "outputId": "7a0df8d8-099b-4bae-a2c8-987b5a2c940d"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=35,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Flow validation images in batches of 32 using val_datagen generator\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=35,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 809 images belonging to 4 classes.\n",
            "Found 316 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWEB2u6Jo63W"
      },
      "source": [
        "When I used 80%/20% splitting the results were:\n",
        "\n",
        "Found 900 images belonging to 4 classes.\n",
        "\n",
        "Found 225 images belonging to 4 classes.\n",
        "\n",
        "When I used 70%30%:\n",
        "\n",
        "Found 809 images belonging to 4 classes.\n",
        "\n",
        "Found 316 images belonging to 4 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gre9Hvf7oU4N"
      },
      "source": [
        "# Part 2: CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnW7ygz_zs9e",
        "outputId": "e9baf747-7305-49ac-aa14-05900794babb"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "#I chose to use the CNN from targil 5 with modifications,\n",
        "#the main one is using softmax in output layer instead of sigmoid\n",
        "\n",
        "# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
        "# the three color channels: R, G, and B\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Input(shape=(150, 150, 3)),\n",
        "        # First convolution extracts 16 filters that are 3x3\n",
        "        # Convolution is followed by max-pooling layer with a 2x2 window\n",
        "        layers.Conv2D(16, 3, activation='relu'),\n",
        "        layers.MaxPooling2D(2),\n",
        "        # Second convolution extracts 32 filters that are 3x3\n",
        "        # Convolution is followed by max-pooling layer with a 2x2 window\n",
        "        layers.Conv2D(32, 3, activation='relu'),\n",
        "        layers.MaxPooling2D(2),\n",
        "        # Third convolution extracts 64 filters that are 3x3\n",
        "        # Convolution is followed by max-pooling layer with a 2x2 window\n",
        "        layers.Conv2D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling2D(2),\n",
        "        # Flatten feature map to a 1-dim tensor so we can add fully connected layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(4, activation='softmax'), #4 categories\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTQzLe7H9elb",
        "outputId": "0da7c758-7250-47cf-fd8b-6331fc91d320"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 146, 146, 16)      1216      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 73, 73, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 71, 71, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 35, 35, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 33, 33, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               8389120   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 8,415,524\n",
            "Trainable params: 8,415,524\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC-edswvykG-"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop,Adam, SGD\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=SGD(lr=0.001,momentum=0.9),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhxfxmdYqagc"
      },
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=len(train_generator),\n",
        "\t\tvalidation_data=validation_generator, validation_steps=len(validation_generator), epochs=24, verbose=2)\n",
        "\t# evaluate model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f-x4FP-qeZC"
      },
      "source": [
        "_, acc = model.evaluate_generator(validation_generator, steps=len(validation_generator), verbose=0)\n",
        "print('> %.3f' % (acc * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuD-yqH0_hL6"
      },
      "source": [
        "Results:\n",
        "\n",
        "First run: Batch size=32, number of epochs=15, dropout=0.5, optimizer=RMS with lr=0.001, accuracy= **91.15%**\n",
        "\n",
        "Second run: Batch size=20, number of epochs=10, dropout=0.5, optimizer=RMS with lr=0.001, accuracy= **94.69%**\n",
        "\n",
        "Third run: Batch size=20, number of epochs=10, dropout=0.5, optimizer=RMS with lr=0.01, accuracy= **63.274%**\n",
        "\n",
        "Fourth run: Batch size=20, number of epochs=10, dropout=0.25, optimizer=RMS with lr=0.001, accuracy= **92.444%**\n",
        "\n",
        "Fifth run: Batch size=40, number of epochs=20, dropout=0.5, optimizer=RMS with lr=0.001, data splitting 70%/30%, accuracy= **93.038%**\n",
        "\n",
        "Sixth run: Batch size=35, number of epochs=20, dropout=0.5, optimizer=Adam with lr=0.001 and beta_1=0.9, data splitting 70%/30%, accuracy= **94.937%**\n",
        "\n",
        "Seventh run: Batch size=35, number of epochs=25, dropout=0.5, optimizer=SGD with lr=0.001 and momentum=0.9, data splitting 70%/30%, accuracy= **94.62%**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13SEriT93APl"
      },
      "source": [
        "# Part 3: Adding data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr0rTxXeADeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6dcf43e-1feb-40d7-e048-10611cc56f75"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# Adding rescale, rotation_range, width_shift_range, height_shift_range,\n",
        "# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=38,\n",
        "    width_shift_range=0.4, #previous value: 0.2\n",
        "    height_shift_range=0.2, #previous value: 0.2\n",
        "    shear_range=0.35, #previous value: 0.2\n",
        "    zoom_range=0.27, #previous value: 0.2\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=35,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Flow validation images in batches of 32 using val_datagen generator\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=35,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 900 images belonging to 4 classes.\n",
            "Found 225 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8pfqr2rQ7vZ"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop,Adam,SGD\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=SGD(lr=0.0018,momentum=0.9),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvWJr7EbFaXS"
      },
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=len(train_generator),\n",
        "\t\tvalidation_data=validation_generator, validation_steps=len(validation_generator), epochs=24, verbose=2)\n",
        "\t# evaluate model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTFDRkMsMp5c"
      },
      "source": [
        "_, acc = model.evaluate_generator(validation_generator, steps=len(validation_generator), verbose=0)\n",
        "print('> %.3f' % (acc * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBYHHlIcMsxx"
      },
      "source": [
        "Results:\n",
        "\n",
        "First run: Batch size=20, number of epochs=15, dropout=0.5, optimizer=RMS with lr=0.001, accuracy= **92.889%**\n",
        "\n",
        "Second run: Batch size=32, number of epochs=10, dropout=0.5, optimizer=RMS with lr=0.001, changes made to values in the augmentation definition, accuracy= **93.778%**\n",
        "\n",
        "Third run: same as second but with batch size=20, accuracy=**92.889%**\n",
        "\n",
        "Fourth run: same as third but with batch size=32, optimizer=Adam, accuracy=**93.778%**\n",
        "\n",
        "Fifth run: Batch size=35, number of epochs=25, dropout=0.5, optimizer=Adam, data splitting 70%/30%, accuracy= **92.722%**\n",
        "\n",
        "Fifth run: Batch size=35, number of epochs=25, dropout=0.5, optimizer=SGD, data splitting 70%/30%, accuracy= **95.886%**\n",
        "\n",
        "Sixth run: Batch size=35, number of epochs=24, dropout=0.5, optimizer=SGD, data splitting 70%/30%, I changed the values of the augmentation and SGD, accuracy= **96.519%**\n",
        "\n",
        "Conclusion: Data augmentation improves results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHcpF-up3Kok"
      },
      "source": [
        "# Part 4: Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-89_fZMTHQD",
        "outputId": "d48bf584-d3c4-47cc-a751-34c85d170fa3"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#Using data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "     rotation_range=43,\n",
        "    width_shift_range=0.24, #previous value: 0.2\n",
        "    height_shift_range=0.125, #previous value: 0.2\n",
        "    shear_range=0.55, #previous value: 0.2\n",
        "    zoom_range=0.15, #previous value: 0.2\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(224,224),  # All images will be resized to 224x224 for VGG\n",
        "        batch_size=12,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Flow validation images in batches of 32 using val_datagen generator\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(224,224),\n",
        "        batch_size=9,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 900 images belonging to 4 classes.\n",
            "Found 225 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YwHo96vz1kA"
      },
      "source": [
        "#Transfer learning with VGG16\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten,Dropout\n",
        "from tensorflow.keras.optimizers import SGD,Adam,RMSprop\n",
        "model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "\t# mark loaded layers as not trainable\n",
        "for layer in model.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\t# add new classifier layers\n",
        "dropout1= Dropout(0.15)(model.layers[-1].output) #adding dropout\n",
        "flat1 = Flatten()(dropout1)\n",
        "class1 = Dense(512, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "output = Dense(4, activation='softmax')(class1) #softmax activation in output layer\n",
        "# define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "# compile model\n",
        "opt =Adam( lr=0.0018,beta_1=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci3szQ7BTdHa"
      },
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=len(train_generator),\n",
        "\t\tvalidation_data=validation_generator, validation_steps=len(validation_generator), epochs=40, verbose=2)\n",
        "\t# evaluate model\n",
        "_, acc = model.evaluate_generator(validation_generator, steps=len(validation_generator), verbose=0)\n",
        "print('> %.3f' % (acc * 100.0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHieW4UNVJkN"
      },
      "source": [
        "Results:\n",
        "\n",
        "First run: batch size=20, number of epochs=10, dropout=0, optimizer=SGD with lr=0.001 and momentum=0.9, with augmentation, accuracy= **91.111%**\n",
        "\n",
        "Second run: batch size=20, number of epochs=10, dropout=0.25, optimizer=SGD with lr=0.001 and momentum=0.9, with augmentation, accuracy= **90.222%**\n",
        "\n",
        "Third run: same as second but no dropout and no augmentation, accuracy= **88.444%**\n",
        "\n",
        "Fourth run: batch size=20, number of epochs=15, dropout=0, optimizer=SGD with lr=0.001 and momentum=0.9, with augmentation, accuracy=**92.444%**\n",
        "\n",
        "Fifth run: batch size=20, number of epochs=15, dropout=0, optimizer=Adam with lr=0.001 and beta_1=0.9, with augmentation, accuracy= **93.778%**\n",
        "\n",
        "Sixth run: same as fifth with batch size=32, epochs=20, accuracy= **95.111%**\n",
        "\n",
        "Seventh run: same as sixth, 22 epochs,accuracy= **95.556%**\n",
        "\n",
        "Eighth run: same as sixth but optimizer=RMSprop, accuracy=**92.444%**\n",
        "\n",
        "Ninth run: batch size=32, number of epochs=22, dropout=0, optimizer=Adam with lr=0.001 and beta_1=0.9, with augmentation, with data splitting 70%/30% instead of 20%/80%, accuracy= **91.139%**\n",
        "\n",
        "Tenth run: batch size=25, number of epochs=20, dropout=0.25, optimizer=Adam with lr=0.0018 and beta_1=0.92, with augmentation, with data splitting 80%/20%, accuracy= **95.556%**\n",
        "\n",
        "Eleventh run: batch size=11, number of epochs=35, dropout=0.15, optimizer=Adam with lr=0.0018 and beta_1=0.92, with augmentation, with data splitting 80%/20%, accuracy= **96%**"
      ]
    }
  ]
}